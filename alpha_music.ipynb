{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d49b7eaa-0bf7-48e0-9fe6-51ba03036ba1",
    "_uuid": "9dacb7a2a5a9e582d7e597be4ea479cedd79489a"
   },
   "source": [
    "# Classifying multi-label comments with Logistic Regression\n",
    "#### Rhodium Beng\n",
    "Started on 20 December 2017\n",
    "\n",
    "This kernel is inspired by:\n",
    "- kernel by Jeremy Howard : _NB-SVM strong linear baseline + EDA (0.052 lb)_\n",
    "- kernel by Issac : _logistic regression (0.055 lb)_\n",
    "- _Solving Multi-Label Classification problems_, https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "8f6a95ee-cc95-4c9f-a8f7-72ae58ec13d6",
    "_uuid": "5c5f4cc8865644748e11336736bbe584adebe7b1",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "80d61838-9025-4cba-bb0e-58175586b21b",
    "_uuid": "4f65d03ddbfd127307d3e415003346eb898b4d6b"
   },
   "source": [
    "## Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "094fff47-db10-447c-965e-08056f718bde",
    "_uuid": "4e35cd5fcae1581dbd6bc51f14728e27fe63fe70",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/train.csv')\n",
    "test_df = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "09986b08-eda6-4438-9cbe-52a61d8d57fa",
    "_uuid": "89d1c9a4f9598427e8a20d66fa9e56796ad720f6"
   },
   "source": [
    "## Examine the data (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "c4c7137d-6bc7-4b41-b50a-e511883155e9",
    "_kg_hide-output": true,
    "_uuid": "9e53b7599d707a9420a75c37c7ac6d05bed9df7b"
   },
   "outputs": [],
   "source": [
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "40597119-1274-4d5b-a054-b4b17dbcbb36",
    "_uuid": "6c824d91ae1e801e1489e93e1a9932c8c0cb0e0a"
   },
   "source": [
    "In the training data, the comments are labelled as one or more of the six categories; toxic, severe toxic, obscene, threat, insult and identity hate. This is essentially a multi-label classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "7e29bebb-d9b7-44ab-a6ba-9f98e6507d5e",
    "_uuid": "7f7f2581edb2f42a64812dec31622a011dceff80",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cols_target = ['obscene','insult','toxic','severe_toxic','identity_hate','threat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "ce00e980-da07-4412-ae4c-5152cc2036e0",
    "_uuid": "e4a226272e319458391e117e8fb7f16b17c4884f"
   },
   "outputs": [],
   "source": [
    "# check missing values in numeric columns\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b2be6443-eb4e-4e12-81b3-faf2ea692ca4",
    "_uuid": "474f4cf26190a2f2011ca0908052973fec1b1520"
   },
   "source": [
    "There are no missing numeric values. \n",
    "As the mean values are very small (some way below 0.05), there would be many not labelled as positive in the six categories. From this I guess that there would be many comments which are not labelled in any of the six categories. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "f0a52911f77fade7ece075fdc0d9df9433029eec"
   },
   "outputs": [],
   "source": [
    "unlabelled_in_all = train_df[(train_df['toxic']!=1) & (train_df['severe_toxic']!=1) & (train_df['obscene']!=1) & \n",
    "                            (train_df['threat']!=1) & (train_df['insult']!=1) & (train_df['identity_hate']!=1)]\n",
    "print('Percentage of unlabelled comments is ', len(unlabelled_in_all)/len(train_df)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "997cc605-71a0-4ceb-a64b-e3e44c172aea",
    "_uuid": "99f1db4864c5c538f2a925d7b6733bb4b1c68707"
   },
   "outputs": [],
   "source": [
    "# check for any 'null' comment\n",
    "no_comment = train_df[train_df['comment_text'].isnull()]\n",
    "len(no_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "af5ba625-bd5d-4ad3-948b-5641b10d62fb",
    "_kg_hide-output": true,
    "_uuid": "964e57595bb6e87a47aa82557c9d63aae3c24bd0"
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "6bd65c22-9c8a-4756-a7bf-16187a6044d1",
    "_uuid": "c616bcb0dcf611679dcb5009def9d71d6727cd0e"
   },
   "outputs": [],
   "source": [
    "no_comment = test_df[test_df['comment_text'].isnull()]\n",
    "no_comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ecce8833-b5b4-40b4-a164-015b7380b8b1",
    "_uuid": "ad7f31ff8035dc60d37c8070f8bbaa9e7afac32f"
   },
   "source": [
    "All rows in the training and test data contain comments, so there's no need to clean up null fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "b3dcbd9f-dbb8-4a5a-96b2-b93882516e27",
    "_uuid": "e497fc6688e16602e3f49a11939f32324d295a8d"
   },
   "outputs": [],
   "source": [
    "# let's see the total rows in train, test data and the numbers for the various categories\n",
    "print('Total rows in test is {}'.format(len(test_df)))\n",
    "print('Total rows in train is {}'.format(len(train_df)))\n",
    "print(train_df[cols_target].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "981dff09-3acf-4014-b38a-22afc02a6654",
    "_uuid": "2ff9ed83ba328872d446add97695285dc49f4165"
   },
   "source": [
    "As mentioned earlier, majority of the comments in the training data are not labelled in one or more of these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "b26588a7-9a7f-4183-98b2-fb94a70bedaa",
    "_uuid": "1e97432af65b6b75b436daabc83bdf57775a59c1",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at the character length for the rows in the training data and record these\n",
    "train_df['char_length'] = train_df['comment_text'].apply(lambda x: len(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "d5ac5111-5bba-44c9-a039-7bb01a5bfd59",
    "_uuid": "448c3492fc2fe24f30bd7b97047d69f16b58ca2f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look at the histogram plot for text length\n",
    "sns.set()\n",
    "train_df['char_length'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5b482de7-5fd1-4ef7-b7b4-2abf84ebdc25",
    "_uuid": "d28a801b4057518f87e20c46a77f9dc8757ab944"
   },
   "source": [
    "Most of the text length are within 500 characters, with some up to 5,000 characters long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f16a287f-27d4-4afa-82a1-dd441c2fd36c",
    "_uuid": "e1e0ecc1df6989b0ee73874f273f0dbbfc4c9d5e"
   },
   "source": [
    "Next, let's examine the correlations among the target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "64164a2c-770f-469d-9020-e91714a9b2a8",
    "_uuid": "fab22b3f850c80a10d665ae43ee09b5107a79887",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data = train_df[cols_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "7fc7803b-a7f1-414d-84fa-d1d2201c8bb7",
    "_kg_hide-output": false,
    "_uuid": "58968a44d8fdb3b93ac57f1ca20f81ffb71d164f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colormap = plt.cm.plasma\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.title('Correlation of features & targets',y=1.05,size=14)\n",
    "sns.heatmap(data.astype(float).corr(),linewidths=0.1,vmax=1.0,square=True,cmap=colormap,\n",
    "           linecolor='white',annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "095fd031-32a0-400b-9357-af0c8f50dd6b",
    "_uuid": "4f9da9651b4e007088b19a06165d0727fb4f4e07"
   },
   "source": [
    "Indeed, it looks like some of the labels are higher correlated, e.g. insult-obscene has the highest at 0.74, followed by toxic-obscene and toxic-insult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "caa00f5d-7e26-48cb-92b3-acc1f1de0aca",
    "_uuid": "b677d6a32b7b72e08ba3b46bce572a223db964de"
   },
   "source": [
    "What about the character length & distribution of the comment text in the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "0993d06b-495e-428a-933a-6ffec6bdcef3",
    "_uuid": "36e4d00a5afc15e6b04ffa1e79421396d051f614",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test_df['char_length'] = test_df['comment_text'].apply(lambda x: len(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "828b9990-d78e-46c7-b011-1c66e6e6be79",
    "_uuid": "1cc05875b88e54b5a1079eafc088207536dea2f3"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(test_df['char_length'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b2bbf246-f098-425a-99e3-2eb2126b975c",
    "_uuid": "db69eb685cd1be44de2224c399cbdeabb5ceaa06"
   },
   "source": [
    "Now, the shape of character length distribution looks similar between the training data and the train data. For the training data, I guess the train data were clipped to 5,000 characters to facilitate the folks who did the labelling of the comment categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fdf9d2f6-d248-452f-8f94-562755e3a3f3",
    "_uuid": "d88d9cea99dbd77e81a5b3c4b9309df88b04550b"
   },
   "source": [
    "## Clean up the comment text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "24392feb-0adc-4e27-bd20-41ed8cadce37",
    "_uuid": "b42586552d4cdc79793b0de8e630f863f2b2c456",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "7b426494-c856-4cf4-b0d1-4162bc3aef5b",
    "_uuid": "3944cb9d1311fd3729a7dfb13e8ac8c2309962e5",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# clean the comment_text in train_df [Thanks to Pulkit Jha for the useful pointer.]\n",
    "train_df['comment_text'] = train_df['comment_text'].map(lambda com : clean_text(com))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "dc27cc45-e5a9-4f1e-8d07-e28127afaf97",
    "_uuid": "d56d040c687a15a6d13ba29400372108f1960c1b",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# clean the comment_text in test_df [Thanks, Pulkit Jha.]\n",
    "test_df['comment_text'] = test_df['comment_text'].map(lambda com : clean_text(com))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f5abe72a-13a7-41f6-ae8e-1c34dca97110",
    "_uuid": "cae81f6b1d9bb475fc486d5fbb81981025cc3672"
   },
   "source": [
    "\n",
    "## Define X from entire train & test data for use in tokenization by Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "30bf94de-36aa-4e8d-8d12-64172d8dc446",
    "_uuid": "b15dba3906f67e764af0a627e46d7b7e486888c5",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.drop('char_length',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "49547fd7-9633-4f6a-bd46-84d8966f1e8b",
    "_uuid": "061d59552c6ef83bea8ecf9ffbf203286aeab6f8",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = train_df.comment_text\n",
    "test_X = test_df.comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "083686b8-483a-4fbd-8584-cb9da8357c57",
    "_uuid": "6b1e91df6163a437c5f55e9c4de88dc11a89e5ba"
   },
   "outputs": [],
   "source": [
    "print(X.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b2d898ae-79bc-48b8-8544-fb077f876c67",
    "_uuid": "cc4c2aeef221f5a97e1bd5ea1154052172175351"
   },
   "source": [
    "## Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "9be5a4f2-0e85-4f9a-ac00-b8e9916116cb",
    "_uuid": "7b7adf15d16408eb99689884906d0d687c2f8407"
   },
   "outputs": [],
   "source": [
    "# import and instantiate TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(max_features=5000,stop_words='english')\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "283c1d48-c267-431b-834e-37c8d9222b3c",
    "_uuid": "b755b1d4db58eeb5b0ab668d1aaf4a651d3de441"
   },
   "outputs": [],
   "source": [
    "# learn the vocabulary in the training data, then use it to create a document-term matrix\n",
    "X_dtm = vect.fit_transform(X)\n",
    "# examine the document-term matrix created from X_train\n",
    "X_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "54050711-560a-47cf-b4f9-2fbaf59bc2e4",
    "_uuid": "408301ccb78e3f4056a6d2ebbd239594d1a59da0"
   },
   "outputs": [],
   "source": [
    "# transform the test data using the earlier fitted vocabulary, into a document-term matrix\n",
    "test_X_dtm = vect.transform(test_X)\n",
    "# examine the document-term matrix from X_test\n",
    "test_X_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c84eb9f4-4fa5-418c-883c-9d9316092db0",
    "_uuid": "98a540b42db71f1639d47f31d2a4ea851aa1b9e5"
   },
   "source": [
    "## Solving a multi-label classification problem\n",
    "One way to approach a multi-label classification problem is to transform the problem into separate single-class classifier problems. This is known as 'problem transformation'. There are three methods:\n",
    "* _**Binary Relevance.**_ This is probably the simplest which treats each label as a separate single classification problems. The key assumption here though, is that there are no correlation among the various labels.\n",
    "* _**Classifier Chains.**_ In this method, the first classifier is trained on the input X. Then the subsequent classifiers are trained on the input X and all previous classifiers' predictions in the chain. This method attempts to draw the signals from the correlation among preceding target variables.\n",
    "* _**Label Powerset.**_ This method transforms the problem into a multi-class problem  where the multi-class labels are essentially all the unique label combinations. In our case here, where there are six labels, Label Powerset would in effect turn this into a 2^6 or 64-class problem. {Thanks Joshua for pointing out.}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7abd771a-b0f3-47bc-b4ff-13a78aff48e7",
    "_uuid": "389245398dce9573dfa0f7c2facd2849d2174f1f"
   },
   "source": [
    "## Binary Relevance - build a multi-label classifier using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "e30be87f-e0a6-4fd7-bff2-b3c37737cbfe",
    "_uuid": "e7e5707b19c35c8371a0cff7351bc8aadc33acd1"
   },
   "outputs": [],
   "source": [
    "# import and instantiate the Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "logreg = LogisticRegression(C=12.0)\n",
    "\n",
    "# create submission file\n",
    "submission_binary = pd.read_csv('../input/sample_submission.csv')\n",
    "\n",
    "for label in cols_target:\n",
    "    print('... Processing {}'.format(label))\n",
    "    y = train_df[label]\n",
    "    # train the model using X_dtm & y\n",
    "    logreg.fit(X_dtm, y)\n",
    "    # compute the training accuracy\n",
    "    y_pred_X = logreg.predict(X_dtm)\n",
    "    print('Training accuracy is {}'.format(accuracy_score(y, y_pred_X)))\n",
    "    # compute the predicted probabilities for X_test_dtm\n",
    "    test_y_prob = logreg.predict_proba(test_X_dtm)[:,1]\n",
    "    submission_binary[label] = test_y_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5d0970eb-bc44-4fad-91b7-e2a23c2f986d",
    "_uuid": "ee6d942b861235107e0d94174a8d21c2bad0e9ea"
   },
   "source": [
    "### Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "a2816552-c314-4584-ad49-8464e80b1b29",
    "_uuid": "b89e22016a7b4f282940dc23253cbf343c1fbf83"
   },
   "outputs": [],
   "source": [
    "submission_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "fb1bef7d-586e-437d-a1d3-b0868e7ac321",
    "_uuid": "75d9571cc77eb1805d897af6ca0d86fd28405c6d",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# generate submission file\n",
    "submission_binary.to_csv('submission_binary.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "be8c8e90-f798-46d8-b77c-cd0668292646",
    "_uuid": "18644d484121f3bab7a06207bd7fa739b15feff5"
   },
   "source": [
    "#### Binary Relevance with Logistic Regression classifier scored 0.074 on the public leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5018c7ea-dd27-4d4c-b9ad-ef2140c773e5",
    "_uuid": "0393cd387f508609c0d068c68fb7dbb0be659383"
   },
   "source": [
    "## Classifier Chains - build a multi-label classifier using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "b2172222-42b8-4f0a-8d20-160d52af6f62",
    "_uuid": "f1b4fea94c83661d7bbad5c6bc7a5994643128e2",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create submission file\n",
    "submission_chains = pd.read_csv('../input/sample_submission.csv')\n",
    "\n",
    "# create a function to add features\n",
    "def add_feature(X, feature_to_add):\n",
    "    '''\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    '''\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "20c3ff4a-8925-4c78-a8f1-74f080f0b890",
    "_uuid": "3cdc6de563643c7a86f0c54ecff2f720695fe81d"
   },
   "outputs": [],
   "source": [
    "for label in cols_target:\n",
    "    print('... Processing {}'.format(label))\n",
    "    y = train_df[label]\n",
    "    # train the model using X_dtm & y\n",
    "    logreg.fit(X_dtm,y)\n",
    "    # compute the training accuracy\n",
    "    y_pred_X = logreg.predict(X_dtm)\n",
    "    print('Training Accuracy is {}'.format(accuracy_score(y,y_pred_X)))\n",
    "    # make predictions from test_X\n",
    "    test_y = logreg.predict(test_X_dtm)\n",
    "    test_y_prob = logreg.predict_proba(test_X_dtm)[:,1]\n",
    "    submission_chains[label] = test_y_prob\n",
    "    # chain current label to X_dtm\n",
    "    X_dtm = add_feature(X_dtm, y)\n",
    "    print('Shape of X_dtm is now {}'.format(X_dtm.shape))\n",
    "    # chain current label predictions to test_X_dtm\n",
    "    test_X_dtm = add_feature(test_X_dtm, test_y)\n",
    "    print('Shape of test_X_dtm is now {}'.format(test_X_dtm.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d917c9c5-5140-4b08-86b3-64e0b566bc1b",
    "_uuid": "a0be9c78f3facd8215c0d83c446dcef61af3fc43"
   },
   "source": [
    "### Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "4b7db16c-ca27-4fa8-933c-369711f60ea3",
    "_uuid": "f1d85021f64b145d28a55f65e0d3e806a6c91625"
   },
   "outputs": [],
   "source": [
    "submission_chains.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "3e1dfebe-ecb4-4a0e-958c-56a2d27e6a7d",
    "_uuid": "da35f9fd53f310366d0b760eb7573ae1b4e122c4",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# generate submission file\n",
    "submission_chains.to_csv('submission_chains.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6f76bfb5-0ad1-42f3-8481-1dc6226a13b9",
    "_uuid": "ecdadaa139a91703da20aab1f5ee77b25023bab4"
   },
   "source": [
    "## Create a combined submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "1a5fd709-81a7-4373-ad8a-45b61d4c1167",
    "_uuid": "b056f497fdcc13a46d5c83d988081dc2e5e99f1d",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create submission file\n",
    "submission_combined = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "62d78056-24e4-409c-a5dd-221b1c9f3b2b",
    "_uuid": "0bb5e1b109a8b043801e5b6360fb858e233a9f2c"
   },
   "source": [
    "Combine using simple average from Binary Relevance and Classifier Chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "650d5945-f32f-49a9-b5ac-e932cf64e0dd",
    "_uuid": "81258bf73f51973d923f2695d81cb8523d21b4a0",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# corr_targets = ['obscene','insult','toxic']\n",
    "for label in cols_target:\n",
    "    submission_combined[label] = 0.5*(submission_chains[label]+submission_binary[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_cell_guid": "3b7c66fd-d82e-46f1-bb41-004ed63c2569",
    "_uuid": "1aeb3dbed4d2af2e1125edc00d5d699e71745cf4"
   },
   "outputs": [],
   "source": [
    "submission_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "3af7d1e1-1d08-43b8-bdb1-0740dd4bdbbd",
    "_uuid": "eded1209e8bb777308d209c161d18c62c8a0c1ef",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# generate submission file\n",
    "submission_combined.to_csv('submission_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7a012382-ad72-4c76-b53c-1f756544e23e",
    "_uuid": "cbbce014f73f54e9bf3e88e096ad66b89b88fcf4"
   },
   "source": [
    "### Thanks for reading my kernel.\n",
    "### Tips and comments are most welcomed & appreciated.\n",
    "### Please upvote if you find it useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2b370cff-d22e-4393-94d5-98b4883a32d8",
    "_uuid": "21a21a9ff3c174753c8d48495e058a1782bf7a76",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
